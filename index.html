<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="description" content="Design & Learning Research Group Submission to euROBIN MSVC @ IROS 2024" />
  <meta name="keywords" content="Design & Learning Research Group, euROBIN, MSVC, IROS 2024" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>
    Design & Learning Research Group Submission - euROBIN MSVC @ IROS 2024
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="css/bulma.min.css" />
  <link rel="stylesheet" href="css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="css/bulma-slider.min.css" />
  <link rel="stylesheet" href="css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="css/index.css" />
  <link rel="icon" href="images/icon.jpg" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-3 publication-title">
              euROBIN MSVC @ IROS 2024
            </h2>
            <h1 class="title is-2 publication-title">
              Design & Learning Research Group Submission
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"> Ning Guo, </span>
              <span class="author-block"> Xudong Han </span>
              <span class="author-block"> Haoran Sun, </span>
              <span class="author-block"> Chaoyang Song*, </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://bionicdl.ancorasir.com">
                  Design and Learning Research Group
                </a>
                @ <a href="https://www.sustech.edu.cn/en">SUSTech</a>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://youtu.be/6FlQ3zCi53w" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ancorasir/DesignLearnRG_euROBIN"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Data Link. -->
                <span class="link-block">
                  <a href="https://drive.google.com/drive/folders/1yhIDuubmL7S85B2c-S1hvZppHv4G-Wow?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- zh-CN Link. -->
                <span class="link-block">
                  <a href="https://msvc-dlrg.github.io/zh-cn" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-globe"></i>
                    </span>
                    <span>中文</span>
                  </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="images/teaser.png" alt="Teaser" width="100%" />
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              We present a solution for the euROBIN MSVC @ IROS 2024 competition.
              To build up the robotics system, we use UR10e Cobot along with
              commercially available parts, and designed fingers which can
              robustify the manipulation of the probe and cables. To complete
              the tasks, we collect images of the task board and fine-tuned
              YOLOv8 to detect the key elements on the board (red/blue
              buttons, red test port) and estimate 6D pose of the board in
              robot base frame. For each task, we design a sequence of gripper
              motions relative to the task board frame to make the task
              execution invariant to the global pose of the board. Using the
              estimated 6D pose of the board, we then convert all the motions
              to robot base frame and send command to UR10e.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/6FlQ3zCi53w?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Robot Platform</h2>
          <div class="content has-text-justified">
            <p>
              The robot system used in the competition consists of a UR10e
              collebrative robot, a Robotiq Hand-E gripper, a pair of
              3D-printed fingertips, a Realsense D435i RGB-D camera, a camera
              bracket, and a ZHIYUN FIVERAY M20 fill light.
            </p>
          </div>
          <table align="center" width="80%" border="1">
            <tr>
              <th width="20%">Equipment</th>
              <th width="30%">Image</th>
              <th width="50%">Description</th>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.universal-robots.com/products/ur10-robot">
                  UR10e Cobot
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="images/ur10e.png" alt="UR10e" width="60%" />
              </td>
              <td style="vertical-align: middle">
                The UR10e cobot is mounted with the table and controlled
                through the real-time data exchange (RTDE) interface by the
                laptop.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://robotiq.com/products/adaptive-grippers#Hand-E">
                  Robotiq Hand-E gripper
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="images/hand-e.png" alt="Hand-E" width="40%" />
              </td>
              <td style="vertical-align: middle">
                The Hand-E gripper is mounted on the tool flange of the UR10e
                cobot and controlled via the Modbus RTU RS485 protocol.
              </td>
            </tr>
            <tr>
              <td style="vertical-align: middle">
                <a href="https://cad.onshape.com/documents/43edc50e275c72eace7a4839">
                  3D-printed fingertips
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="images/fingertip.png" alt="3D-printed fingertips" width="70%" />
              </td>
              <td style="vertical-align: middle">
                The fingertips are re-designed based on the original
                fingertips of the Robotiq Hand-E gripper to adapt to the
                competition tasks by adding grooves. They are 3D-printed with
                nylon (HP PA12).
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.intelrealsense.com/depth-camera-d435i">
                  Intel Realsense D435i camera
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="images/d435i.png" alt="Realsense D435i" width="80%" />
              </td>
              <td style="vertical-align: middle">
                The Realsense D435i RGB-D camera is mounted on the UR10e cobot
                through a bracket and connected to the laptop through a USB
                cable.
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://cad.onshape.com/documents/01d4267b0af8aab9d6acb1ab">
                  Camera bracket
                </a>
              </td>
              <td style="vertical-align: middle">
                <img src="images/camera_bracket.png" alt="Camera bracket" width="80%" />
              </td>
              <td style="vertical-align: middle">
                The camera bracket is designed to mount the Realsense D435i
                RGB-D camera on the UR10e cobot. It is frabricated by CNC with
                Al6061.
              </td>
            </tr>

            <tr>
              <td style="vertical-align: middle">
                <a href="https://www.zhiyun-tech.com/en/product/detail/867">
                  ZHIYUN FIVERAY M20 fill light
                </a>
                (Optinal)
              </td>
              <td style="vertical-align: middle">
                <img src="images/fiveray-m20.png" alt="FIVERAY M20" width="70%" />
              </td>
              <td style="vertical-align: middle">
                The ZHIYUN FIVERAY M20 fill light is mounted on the camera to
                provide sufficient illumination. It is up to 2010 Lux (0.5m)
                and able to work for more than 40 minutes without charging.
              </td>
            </tr>
          </table>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Solution</h2>
          <h3 class="title is-4">Task 1-1: Task Board Pose Estimation</h3>
          <div class="content has-text-justified">
            <p>
              The task board is placed at a random orientation on two velcro
              strips to hold it in place for a trial. Therefore, for all
              subsequent tasks, the task board needs to be localized. We use
              fine-tuned YOLOv8 to detect the red button, blue button and the
              red test port. As the positional relationship between these task
              board elements is known and fixed, we can estimate the global 6D
              pose (relative to the robot base) of the board.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column is-half" style="width: 40%;">
              <img src="images/pose_yolo.png" alt="Task Board Pose Estimation" width="100%" />
            </div>
            <div class="column is-half" style="width: 40%;">
              <img src="images/pose_ref.png" alt="Task Board Pose Relative to Robot Base" width="100%" />
            </div>
          </div>
          <h3 class="title is-4">Task 1-2: Press Blue Button</h3>
          <div class="content has-text-justified">
            <p>
              To press the button, we first command the gripper to go right
              over it and then lower the gripper position. Then the gripper
              goes upward, preparing for the next task.
            </p>
          </div>
          <div class="content">
            <img src="images/blue_button.png" alt="Press Blue Button" width="80%" />
          </div>
          <h3 class="title is-4">
            Task 2: Move Slider to Setpoints on Screen
          </h3>
          <div class="content has-text-justified">
            <p>
              After the gripper approaches to and securely grips the slider,
              we detect the screen using another fine-tuned YOLOv8. The pixels
              of the triangles displayed on the screen are extracted based on
              their color to calculate the pixel distance error, which is then
              projected to the actual distance to move along the slider track.
            </p>
          </div>
          <div class="content">
            <img src="images/slider_screen.png" alt="Screen Detection by YOLOv8" width="40%" />
            <video id="teaser" autoplay muted loop playsinline width="40%">
              <source src="videos/move_slider.mp4" type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4">Task 3: Plug in Probe into Test Port</h3>
          <div class="content has-text-justified">
            <p>
              We first move the gripper above the left probe, lower it down to
              grip it. Although we do not estimate the precise orientation of
              the probe, we find it robust enough to set a fixed rotatioio of
              the gripper, since the probe orientation will be aligned with
              the gripper after the gripper closes. To increase the
              determinism of the cable wrapping task, we rotate the probe to a
              known angle after plug it into the test port.
            </p>
          </div>
          <div class="content">
            <img src="images/plug_in_probe.png" alt="Plug in Probe into Test Port" width="80%" />
          </div>
          <h3 class="title is-4">Task 4: Open Door, Probe Circuit</h3>
          <div class="content has-text-justified">
            <p>
              To speed up the task execution, the gripper first pulls out the
              probe, insert the probe tip into the gap of the door edges to
              open it. Then the probe is inserted to the testing slot and is
              put back to the probe slot. The task is challenging as the probe
              may change its pose due to the contact with the task board, but
              the grooves of the fingertips we design helps the gripper to
              securely grasp the probe, dramatically increase the accuracy and
              robustness of the manipulation process.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline width="80%">
              <source src="videos/probe_circuit.mp4" type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4">Task 5: Wrap Cable, Replace Probe</h3>
          <div class="content has-text-justified">
            <p>
              To wrap the cable, we collect the gripper poses from human
              demonstrated motions, and transform the poses to the task board
              frame, making it invariant to the absolute position and
              orientation of the board.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline width="80%">
              <source src="videos/wrap_cable.mp4" type="video/mp4">
            </video>
          </div>
          <h3 class="title is-4">Task 6: Press Stop Trial Button</h3>
          <div class="content has-text-justified">
            <p>
              Pressing the stop trial button (red button) is similar as
              pressing the blue button. Finally, the robot gripper Presses the
              red button to finish the trial.
            </p>
          </div>
          <div class="content">
            <img src="images/stop_trial_button.png" alt="Press Stop Trial Button" width="80%" />
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Continuous Trials</h2>
          <div class="content has-text-justified">
            <p>
              Here are 5 continuous trials with random initial poses of the
              task board, showing the robustness of our solution.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline controls width="80%">
              <source src="videos/continuous_trials.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Fastest Trial</h2>
          <div class="content has-text-justified">
            <p>
              Here is the fastest trial we have achieved according to the record
              by the web board. The total trial time is 28.17s.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline controls width="80%">
              <source src="videos/fastest_trial.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Skill Transferability</h2>
          <div class="content has-text-justified">
            <p>
              We transfer the skill to battery extraction of smoke alarm. The
              battery's pose is detected using YOLOv8 with the image captured
              by the camera. The gripper go through the demonstrated trajectories
              to lift up and grasp the battery and place beside the smoke alarm.
              To pick up the new battery, the robot goes above it and the pose is
              also detected by YOLOv8. The gripper pick up the battery and place into
              the containers.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline controls width="80%">
              <source src="videos/battery_extraction.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template credit to
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, and is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons
                Attribution-ShareAlike 4.0 International
                License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>